{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a350234",
   "metadata": {},
   "source": [
    "# Neural Computing Individual Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d6fc1f",
   "metadata": {},
   "source": [
    "In this project, we are going to be comparing RNNs and CNNs in analysing algorithmic trading data. We will use one of the biggest datasets on Nifty 50 algorithmic trading dataset, analysing how these algorithms do againt 4-5 different companies. This dataset contains data on 102 companies, for 1 minute, 3 minute, 5 minute, 10 minute, 15 minute, 30 minute, 1 hour and 1 day, with separate datasets for each of these time-scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340be83b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0718a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import skorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8776e",
   "metadata": {},
   "source": [
    "## Data Loading and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b8822e",
   "metadata": {},
   "source": [
    "We evaluate the our CNNs and RNNs on three different companies from the NIFTY 50 stock exchange."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cbf4b0",
   "metadata": {},
   "source": [
    "## ICICI Bank Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02fb776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ICICI_min = pd.read_csv(r'C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\ICICI Bank\\ICICIBANK_minute_data.csv\\ICICIBANK_minute_data.csv')\n",
    "ICICI_3min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\ICICI Bank\\ICICIBANK_3minute_data.csv\\ICICIBANK_3minute_data.csv\")\n",
    "ICICI_5min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\ICICI Bank\\ICICIBANK_5minute_data.csv\\ICICIBANK_5minute_data.csv\")\n",
    "ICICI_10min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\ICICI Bank\\ICICIBANK_10minute_data.csv\\ICICIBANK_10minute_data.csv\")\n",
    "ICICI_15min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\ICICI Bank\\ICICIBANK_15minute_data.csv\\ICICIBANK_15minute_data.csv\")\n",
    "ICICI_30min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\ICICI Bank\\ICICIBANK_30minute_data.csv\\ICICIBANK_30minute_data.csv\")\n",
    "ICICI_60min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\ICICI Bank\\ICICIBANK_60minute_data.csv\")\n",
    "ICICI_day = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\ICICI Bank\\ICICIBANK_day_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e898780",
   "metadata": {},
   "source": [
    "## AXIS Bank Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad6dc621",
   "metadata": {},
   "outputs": [],
   "source": [
    "AXIS_min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\AXIS Bank\\AXISBANK_minute_data.csv\\AXISBANK_minute_data.csv\")\n",
    "AXIS_3min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\AXIS Bank\\AXISBANK_3minute_data.csv\\AXISBANK_3minute_data.csv\")\n",
    "AXIS_5min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\AXIS Bank\\AXISBANK_5minute_data.csv\\AXISBANK_5minute_data.csv\")\n",
    "AXIS_10min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\AXIS Bank\\AXISBANK_10minute_data.csv\\AXISBANK_10minute_data.csv\")\n",
    "AXIS_15min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\AXIS Bank\\AXISBANK_15minute_data.csv\\AXISBANK_15minute_data.csv\")\n",
    "AXIS_30min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\AXIS Bank\\AXISBANK_30minute_data.csv\\AXISBANK_30minute_data.csv\")\n",
    "AXIS_60min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\AXIS Bank\\AXISBANK_60minute_data.csv\")\n",
    "AXIS_day = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\AXIS Bank\\AXISBANK_day_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95130eb9",
   "metadata": {},
   "source": [
    "## TATASTEEL Bank Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a409b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "TATASTEEL_min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\TATASTEEL Bank\\TATASTEEL_minute_data.csv\\TATASTEEL_minute_data.csv\")\n",
    "TATASTEEL_3min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\TATASTEEL Bank\\TATASTEEL_3minute_data.csv\\TATASTEEL_3minute_data.csv\")\n",
    "TATASTEEL_5min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\TATASTEEL Bank\\TATASTEEL_5minute_data.csv\\TATASTEEL_5minute_data.csv\")\n",
    "TATASTEEL_10min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\TATASTEEL Bank\\TATASTEEL_10minute_data.csv\\TATASTEEL_10minute_data.csv\")\n",
    "TATASTEEL_15min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\TATASTEEL Bank\\TATASTEEL_15minute_data.csv\\TATASTEEL_15minute_data.csv\")\n",
    "TATASTEEL_30min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\TATASTEEL Bank\\TATASTEEL_30minute_data.csv\\TATASTEEL_30minute_data.csv\")\n",
    "TATASTEEL_60min = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\TATASTEEL Bank\\TATASTEEL_60minute_data.csv\")\n",
    "TATASTEEL_day = pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\Algo Trading KAGGLE\\TATASTEEL Bank\\TATASTEEL_day_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02f614c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'date', 'close', 'high', 'low', 'open', 'volume'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TATASTEEL_day.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00e6407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "sns.heatmap(ICICI_min.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c312eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "sns.heatmap(TATASTEEL_day.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ada6a",
   "metadata": {},
   "source": [
    "There are no null/ duplicate values within this data. Normally stock data/ algorithmic data is clean and does not require much pre-processing. We have to convert the date to datetime format. After that, we can move on to building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e8d88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ICICI_min[\"date\"] = pd.to_datetime(ICICI_min[\"date\"], format='%Y%m%d %H:%M:%S')\n",
    "ICICI_3min[\"date\"] = pd.to_datetime(ICICI_3min[\"date\"], format='%Y%m%d %H:%M:%S')\n",
    "ICICI_5min[\"date\"] = pd.to_datetime(ICICI_5min[\"date\"], format='%Y%m%d %H:%M:%S')\n",
    "ICICI_10min[\"date\"] = pd.to_datetime(ICICI_10min[\"date\"], format='%Y%m%d %H:%M:%S')\n",
    "ICICI_15min[\"date\"] = pd.to_datetime(ICICI_15min[\"date\"], format='%Y%m%d %H:%M:%S')\n",
    "ICICI_30min[\"date\"] = pd.to_datetime(ICICI_30min[\"date\"], format='%Y%m%d %H:%M:%S')\n",
    "ICICI_60min[\"date\"] = pd.to_datetime(ICICI_60min[\"date\"], format='%Y%m%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ca51cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-02-02 09:15:00+05:30</td>\n",
       "      <td>324.09</td>\n",
       "      <td>327.27</td>\n",
       "      <td>324.00</td>\n",
       "      <td>326.68</td>\n",
       "      <td>432718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-02-02 09:16:00+05:30</td>\n",
       "      <td>323.64</td>\n",
       "      <td>324.09</td>\n",
       "      <td>323.27</td>\n",
       "      <td>323.82</td>\n",
       "      <td>365505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-02-02 09:17:00+05:30</td>\n",
       "      <td>323.86</td>\n",
       "      <td>323.95</td>\n",
       "      <td>323.32</td>\n",
       "      <td>323.64</td>\n",
       "      <td>165473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-02-02 09:18:00+05:30</td>\n",
       "      <td>323.50</td>\n",
       "      <td>324.09</td>\n",
       "      <td>323.36</td>\n",
       "      <td>324.09</td>\n",
       "      <td>225438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-02-02 09:19:00+05:30</td>\n",
       "      <td>322.68</td>\n",
       "      <td>323.59</td>\n",
       "      <td>322.55</td>\n",
       "      <td>323.59</td>\n",
       "      <td>282511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671085</th>\n",
       "      <td>671085</td>\n",
       "      <td>2022-10-24 19:10:00+05:30</td>\n",
       "      <td>928.30</td>\n",
       "      <td>928.45</td>\n",
       "      <td>928.00</td>\n",
       "      <td>928.45</td>\n",
       "      <td>42263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671086</th>\n",
       "      <td>671086</td>\n",
       "      <td>2022-10-24 19:11:00+05:30</td>\n",
       "      <td>929.30</td>\n",
       "      <td>929.65</td>\n",
       "      <td>928.35</td>\n",
       "      <td>928.40</td>\n",
       "      <td>78049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671087</th>\n",
       "      <td>671087</td>\n",
       "      <td>2022-10-24 19:12:00+05:30</td>\n",
       "      <td>929.50</td>\n",
       "      <td>930.00</td>\n",
       "      <td>929.10</td>\n",
       "      <td>929.30</td>\n",
       "      <td>138507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671088</th>\n",
       "      <td>671088</td>\n",
       "      <td>2022-10-24 19:13:00+05:30</td>\n",
       "      <td>931.05</td>\n",
       "      <td>931.10</td>\n",
       "      <td>930.00</td>\n",
       "      <td>930.00</td>\n",
       "      <td>89474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671089</th>\n",
       "      <td>671089</td>\n",
       "      <td>2022-10-24 19:14:00+05:30</td>\n",
       "      <td>933.30</td>\n",
       "      <td>933.30</td>\n",
       "      <td>931.00</td>\n",
       "      <td>931.35</td>\n",
       "      <td>112467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>671090 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                      date   close    high     low    open  \\\n",
       "0                0 2015-02-02 09:15:00+05:30  324.09  327.27  324.00  326.68   \n",
       "1                1 2015-02-02 09:16:00+05:30  323.64  324.09  323.27  323.82   \n",
       "2                2 2015-02-02 09:17:00+05:30  323.86  323.95  323.32  323.64   \n",
       "3                3 2015-02-02 09:18:00+05:30  323.50  324.09  323.36  324.09   \n",
       "4                4 2015-02-02 09:19:00+05:30  322.68  323.59  322.55  323.59   \n",
       "...            ...                       ...     ...     ...     ...     ...   \n",
       "671085      671085 2022-10-24 19:10:00+05:30  928.30  928.45  928.00  928.45   \n",
       "671086      671086 2022-10-24 19:11:00+05:30  929.30  929.65  928.35  928.40   \n",
       "671087      671087 2022-10-24 19:12:00+05:30  929.50  930.00  929.10  929.30   \n",
       "671088      671088 2022-10-24 19:13:00+05:30  931.05  931.10  930.00  930.00   \n",
       "671089      671089 2022-10-24 19:14:00+05:30  933.30  933.30  931.00  931.35   \n",
       "\n",
       "        volume  \n",
       "0       432718  \n",
       "1       365505  \n",
       "2       165473  \n",
       "3       225438  \n",
       "4       282511  \n",
       "...        ...  \n",
       "671085   42263  \n",
       "671086   78049  \n",
       "671087  138507  \n",
       "671088   89474  \n",
       "671089  112467  \n",
       "\n",
       "[671090 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ICICI_min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a07e0",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716cfcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,8))\n",
    "plt.title(\"Closing Price History\")\n",
    "plt.plot(ICICI_min[\"date\"], ICICI_min[\"close\"])\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Closing Price (INR)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba56277",
   "metadata": {},
   "source": [
    "## Preparing Data for Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402506fb",
   "metadata": {},
   "source": [
    "### Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc8a3db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def format_dataframe(df):\n",
    "    # Select columns of interest\n",
    "    df = df[['open', 'high', 'low', 'close']]\n",
    "    \n",
    "    # Apply min-max scaler to selected columns\n",
    "    scaler = MinMaxScaler()\n",
    "    df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
    "    \n",
    "    #Drop other columns\n",
    "    df = df.drop(columns=['Unnamed: 0', 'date', 'volume'], errors = 'ignore')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e22d80cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n"
     ]
    }
   ],
   "source": [
    "ICICI_min = format_dataframe(ICICI_min)\n",
    "ICICI_3min = format_dataframe(ICICI_3min) \n",
    "ICICI_5min = format_dataframe(ICICI_5min)\n",
    "ICICI_10min = format_dataframe(ICICI_10min)\n",
    "ICICI_15min = format_dataframe(ICICI_15min)\n",
    "ICICI_30min = format_dataframe(ICICI_30min)\n",
    "ICICI_60min = format_dataframe(ICICI_60min)\n",
    "ICICI_day = format_dataframe(ICICI_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "988d1fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n"
     ]
    }
   ],
   "source": [
    "AXIS_min = format_dataframe(AXIS_min)\n",
    "AXIS_3min = format_dataframe(AXIS_3min) \n",
    "AXIS_5min = format_dataframe(AXIS_5min)\n",
    "AXIS_10min = format_dataframe(AXIS_10min)\n",
    "AXIS_15min = format_dataframe(AXIS_15min)\n",
    "AXIS_30min = format_dataframe(AXIS_30min)\n",
    "AXIS_60min = format_dataframe(AXIS_60min)\n",
    "AXIS_day = format_dataframe(AXIS_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0461bbc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n",
      "C:\\Users\\rgs88\\AppData\\Local\\Temp\\ipykernel_5840\\4013003869.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['open', 'high', 'low', 'close']] = scaler.fit_transform(df[['open', 'high', 'low', 'close']].values.reshape(-1, 4))\n"
     ]
    }
   ],
   "source": [
    "TATASTEEL_min = format_dataframe(TATASTEEL_min)\n",
    "TATASTEEL_3min = format_dataframe(TATASTEEL_3min) \n",
    "TATASTEEL_5min = format_dataframe(TATASTEEL_5min)\n",
    "TATASTEEL_10min = format_dataframe(TATASTEEL_10min)\n",
    "TATASTEEL_15min = format_dataframe(TATASTEEL_15min)\n",
    "TATASTEEL_30min = format_dataframe(TATASTEEL_30min)\n",
    "TATASTEEL_60min = format_dataframe(TATASTEEL_60min)\n",
    "TATASTEEL_day = format_dataframe(TATASTEEL_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4f70a2",
   "metadata": {},
   "source": [
    "The data below is used to test the model for teaching purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5022c13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ICICI_min.to_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\NN Final Project\\ICICI_min_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f7755e",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad28dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=[\"close\"]), data[\"close\"], test_size=test_ratio, random_state=random_state)\n",
    "\n",
    "    # Split the remaining data into train and validation sets\n",
    "    remaining_ratio = 1 - test_ratio\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_ratio/remaining_ratio, random_state=random_state)\n",
    "\n",
    "    # Convert the output to pandas DataFrame\n",
    "    X_train = pd.DataFrame(X_train, columns=data.drop(columns=[\"close\"]).columns)\n",
    "    y_train = pd.DataFrame(y_train, columns=[\"close\"])\n",
    "    X_val = pd.DataFrame(X_val, columns=data.drop(columns=[\"close\"]).columns)\n",
    "    y_val = pd.DataFrame(y_val, columns=[\"close\"])\n",
    "    X_test = pd.DataFrame(X_test, columns=data.drop(columns=[\"close\"]).columns)\n",
    "    y_test = pd.DataFrame(y_test, columns=[\"close\"])\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2f84b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, y_train_1, X_val_1, y_val_1, X_test_1, y_test_1 = split_data(ICICI_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ede85a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183161</th>\n",
       "      <td>0.122946</td>\n",
       "      <td>0.123106</td>\n",
       "      <td>0.123859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505988</th>\n",
       "      <td>0.410876</td>\n",
       "      <td>0.410709</td>\n",
       "      <td>0.412166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85366</th>\n",
       "      <td>0.065363</td>\n",
       "      <td>0.065272</td>\n",
       "      <td>0.065888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589322</th>\n",
       "      <td>0.782231</td>\n",
       "      <td>0.781778</td>\n",
       "      <td>0.784836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453853</th>\n",
       "      <td>0.256318</td>\n",
       "      <td>0.256696</td>\n",
       "      <td>0.257391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585921</th>\n",
       "      <td>0.825474</td>\n",
       "      <td>0.825318</td>\n",
       "      <td>0.827089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536903</th>\n",
       "      <td>0.527638</td>\n",
       "      <td>0.527595</td>\n",
       "      <td>0.529564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244622</th>\n",
       "      <td>0.194080</td>\n",
       "      <td>0.194042</td>\n",
       "      <td>0.195117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503708</th>\n",
       "      <td>0.397845</td>\n",
       "      <td>0.397880</td>\n",
       "      <td>0.399230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615364</th>\n",
       "      <td>0.638370</td>\n",
       "      <td>0.637938</td>\n",
       "      <td>0.640526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469762 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            open      high       low\n",
       "183161  0.122946  0.123106  0.123859\n",
       "505988  0.410876  0.410709  0.412166\n",
       "85366   0.065363  0.065272  0.065888\n",
       "589322  0.782231  0.781778  0.784836\n",
       "453853  0.256318  0.256696  0.257391\n",
       "...          ...       ...       ...\n",
       "585921  0.825474  0.825318  0.827089\n",
       "536903  0.527638  0.527595  0.529564\n",
       "244622  0.194080  0.194042  0.195117\n",
       "503708  0.397845  0.397880  0.399230\n",
       "615364  0.638370  0.637938  0.640526\n",
       "\n",
       "[469762 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "957612f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def convert_to_tensors(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    X_train_np = X_train.values\n",
    "    X_val_np = X_val.values\n",
    "    X_test_np = X_test.values\n",
    "    y_train_np = y_train.values\n",
    "    y_val_np = y_val.values\n",
    "    y_test_np = y_test.values\n",
    "\n",
    "    X_train_t = torch.from_numpy(X_train_np).type(torch.Tensor)\n",
    "    X_val_t = torch.from_numpy(X_val_np).type(torch.Tensor)\n",
    "    X_test_t = torch.from_numpy(X_test_np).type(torch.Tensor)\n",
    "    y_train_t = torch.from_numpy(y_train_np).type(torch.Tensor)\n",
    "    y_val_t = torch.from_numpy(y_val_np).type(torch.Tensor)\n",
    "    y_test_t = torch.from_numpy(y_test_np).type(torch.Tensor)\n",
    "\n",
    "    return X_train_t, y_train_t, X_val_t, y_val_t, X_test_t, y_test_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89020a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the data for ICICI_min dataset\n",
    "X_train_t_1, y_train_t_1, X_val_t_1, y_val_t_1, X_test_t_1, y_test_t_1 = convert_to_tensors(X_train_1, y_train_1, X_val_1, y_val_1, X_test_1, y_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccd06ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([469762, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_t_1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38e4b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def create_data_loaders(X_train_t, y_train_t, X_val_t, y_val_t, X_test_t, y_test_t, batch_size=100):\n",
    "    train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75e62fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader1, val_loader1, test_loader1 = create_data_loaders(X_train_t_1, y_train_t_1, X_val_t_1, y_val_t_1, X_test_t_1, y_test_t_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20045a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1590, 0.1588, 0.1599],\n",
      "        [0.7073, 0.7073, 0.7096],\n",
      "        [0.1278, 0.1279, 0.1287],\n",
      "        [0.6773, 0.6769, 0.6787],\n",
      "        [0.6906, 0.6905, 0.6925],\n",
      "        [0.5306, 0.5306, 0.5324],\n",
      "        [0.1508, 0.1509, 0.1518],\n",
      "        [0.1796, 0.1797, 0.1806],\n",
      "        [0.3077, 0.3074, 0.3084],\n",
      "        [0.2513, 0.2510, 0.2512],\n",
      "        [0.1028, 0.1027, 0.1034],\n",
      "        [0.1984, 0.1983, 0.1994],\n",
      "        [0.0782, 0.0780, 0.0788],\n",
      "        [0.1580, 0.1578, 0.1587],\n",
      "        [0.6832, 0.6827, 0.6854],\n",
      "        [0.6965, 0.6968, 0.6989],\n",
      "        [0.3098, 0.3097, 0.3111],\n",
      "        [0.1462, 0.1461, 0.1470],\n",
      "        [0.2089, 0.2087, 0.2094],\n",
      "        [0.0745, 0.0744, 0.0750],\n",
      "        [0.0457, 0.0456, 0.0463],\n",
      "        [0.2142, 0.2143, 0.2147],\n",
      "        [0.5596, 0.5594, 0.5612],\n",
      "        [0.1595, 0.1594, 0.1601],\n",
      "        [0.1663, 0.1662, 0.1670],\n",
      "        [0.7425, 0.7423, 0.7444],\n",
      "        [0.1416, 0.1416, 0.1425],\n",
      "        [0.1119, 0.1117, 0.1125],\n",
      "        [0.1829, 0.1831, 0.1838],\n",
      "        [0.0840, 0.0839, 0.0848],\n",
      "        [0.1588, 0.1587, 0.1593],\n",
      "        [0.0843, 0.0842, 0.0851],\n",
      "        [0.2431, 0.2432, 0.2440],\n",
      "        [0.1281, 0.1282, 0.1290],\n",
      "        [0.6834, 0.6828, 0.6854],\n",
      "        [0.1903, 0.1901, 0.1910],\n",
      "        [0.1636, 0.1634, 0.1642],\n",
      "        [0.3018, 0.3017, 0.3028],\n",
      "        [0.6162, 0.6160, 0.6165],\n",
      "        [0.9274, 0.9272, 0.9304],\n",
      "        [0.6171, 0.6167, 0.6193],\n",
      "        [0.7563, 0.7557, 0.7575],\n",
      "        [0.0139, 0.0139, 0.0140],\n",
      "        [0.2936, 0.2936, 0.2947],\n",
      "        [0.2920, 0.2917, 0.2932],\n",
      "        [0.1133, 0.1132, 0.1139],\n",
      "        [0.0937, 0.0935, 0.0943],\n",
      "        [0.1652, 0.1650, 0.1660],\n",
      "        [0.0836, 0.0835, 0.0843],\n",
      "        [0.1812, 0.1821, 0.1822],\n",
      "        [0.2953, 0.2950, 0.2961],\n",
      "        [0.2360, 0.2358, 0.2368],\n",
      "        [0.1204, 0.1205, 0.1212],\n",
      "        [0.1479, 0.1479, 0.1487],\n",
      "        [0.0276, 0.0290, 0.0283],\n",
      "        [0.7075, 0.7071, 0.7094],\n",
      "        [0.7434, 0.7465, 0.7456],\n",
      "        [0.7034, 0.7030, 0.7055],\n",
      "        [0.3034, 0.3032, 0.3047],\n",
      "        [0.5219, 0.5229, 0.5238],\n",
      "        [0.2972, 0.2974, 0.2983],\n",
      "        [0.0936, 0.0934, 0.0943],\n",
      "        [0.2762, 0.2766, 0.2772],\n",
      "        [0.0634, 0.0633, 0.0641],\n",
      "        [0.8972, 0.8987, 0.9002],\n",
      "        [0.3315, 0.3320, 0.3329],\n",
      "        [0.4882, 0.4878, 0.4897],\n",
      "        [0.2504, 0.2509, 0.2517],\n",
      "        [0.3281, 0.3280, 0.3294],\n",
      "        [0.9187, 0.9185, 0.9215],\n",
      "        [0.0934, 0.0936, 0.0942],\n",
      "        [0.5306, 0.5309, 0.5326],\n",
      "        [0.1612, 0.1612, 0.1622],\n",
      "        [0.2384, 0.2391, 0.2363],\n",
      "        [0.1062, 0.1062, 0.1063],\n",
      "        [0.7215, 0.7209, 0.7229],\n",
      "        [0.9450, 0.9443, 0.9470],\n",
      "        [0.8335, 0.8332, 0.8362],\n",
      "        [0.1592, 0.1596, 0.1584],\n",
      "        [0.2911, 0.2909, 0.2919],\n",
      "        [0.0744, 0.0743, 0.0748],\n",
      "        [0.1963, 0.1961, 0.1962],\n",
      "        [0.0149, 0.0149, 0.0154],\n",
      "        [0.6807, 0.6802, 0.6823],\n",
      "        [0.2776, 0.2774, 0.2788],\n",
      "        [0.3275, 0.3275, 0.3289],\n",
      "        [0.9331, 0.9324, 0.9353],\n",
      "        [0.3512, 0.3509, 0.3522],\n",
      "        [0.0680, 0.0679, 0.0685],\n",
      "        [0.3630, 0.3627, 0.3641],\n",
      "        [0.2442, 0.2439, 0.2452],\n",
      "        [0.9531, 0.9524, 0.9551],\n",
      "        [0.7009, 0.7007, 0.7033],\n",
      "        [0.3075, 0.3072, 0.3082],\n",
      "        [0.1901, 0.1903, 0.1912],\n",
      "        [0.4656, 0.4652, 0.4669],\n",
      "        [0.3980, 0.3978, 0.3990],\n",
      "        [0.1060, 0.1061, 0.1068],\n",
      "        [0.8486, 0.8482, 0.8509],\n",
      "        [0.0549, 0.0548, 0.0554]]) tensor([[0.1591],\n",
      "        [0.7076],\n",
      "        [0.1278],\n",
      "        [0.6768],\n",
      "        [0.6915],\n",
      "        [0.5310],\n",
      "        [0.1509],\n",
      "        [0.1800],\n",
      "        [0.3072],\n",
      "        [0.2503],\n",
      "        [0.1025],\n",
      "        [0.1984],\n",
      "        [0.0782],\n",
      "        [0.1579],\n",
      "        [0.6835],\n",
      "        [0.6972],\n",
      "        [0.3101],\n",
      "        [0.1462],\n",
      "        [0.2083],\n",
      "        [0.0745],\n",
      "        [0.0457],\n",
      "        [0.2142],\n",
      "        [0.5598],\n",
      "        [0.1592],\n",
      "        [0.1663],\n",
      "        [0.7429],\n",
      "        [0.1418],\n",
      "        [0.1119],\n",
      "        [0.1831],\n",
      "        [0.0841],\n",
      "        [0.1584],\n",
      "        [0.0843],\n",
      "        [0.2432],\n",
      "        [0.1282],\n",
      "        [0.6834],\n",
      "        [0.1901],\n",
      "        [0.1636],\n",
      "        [0.3021],\n",
      "        [0.6156],\n",
      "        [0.9282],\n",
      "        [0.6175],\n",
      "        [0.7558],\n",
      "        [0.0134],\n",
      "        [0.2935],\n",
      "        [0.2921],\n",
      "        [0.1131],\n",
      "        [0.0934],\n",
      "        [0.1652],\n",
      "        [0.0836],\n",
      "        [0.1823],\n",
      "        [0.2954],\n",
      "        [0.2358],\n",
      "        [0.1203],\n",
      "        [0.1481],\n",
      "        [0.0290],\n",
      "        [0.7076],\n",
      "        [0.7452],\n",
      "        [0.7039],\n",
      "        [0.3037],\n",
      "        [0.5236],\n",
      "        [0.2975],\n",
      "        [0.0934],\n",
      "        [0.2768],\n",
      "        [0.0634],\n",
      "        [0.9000],\n",
      "        [0.3324],\n",
      "        [0.4883],\n",
      "        [0.2510],\n",
      "        [0.3283],\n",
      "        [0.9196],\n",
      "        [0.0938],\n",
      "        [0.5314],\n",
      "        [0.1613],\n",
      "        [0.2381],\n",
      "        [0.1056],\n",
      "        [0.7217],\n",
      "        [0.9445],\n",
      "        [0.8344],\n",
      "        [0.1575],\n",
      "        [0.2912],\n",
      "        [0.0740],\n",
      "        [0.1956],\n",
      "        [0.0149],\n",
      "        [0.6808],\n",
      "        [0.2776],\n",
      "        [0.3278],\n",
      "        [0.9331],\n",
      "        [0.3513],\n",
      "        [0.0678],\n",
      "        [0.3628],\n",
      "        [0.2443],\n",
      "        [0.9531],\n",
      "        [0.7015],\n",
      "        [0.3070],\n",
      "        [0.1906],\n",
      "        [0.4654],\n",
      "        [0.3976],\n",
      "        [0.1061],\n",
      "        [0.8487],\n",
      "        [0.0547]])\n"
     ]
    }
   ],
   "source": [
    "for data, labels in train_loader1:\n",
    "    print(data,labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a611a40",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc81ad80",
   "metadata": {},
   "source": [
    "Can extend to LSTMs later.\n",
    "- Number of LSTM layers: refers to the number of stacked LSTM layers within the model increasing the number of layers\n",
    "- Hidden State Size: number of neurons in each LSTM layer's hidden state. A larger hidden state can help the model capture info.\n",
    "- Dropout: This hyperparameter controls the amount of reularization applied to LSTMS hidden states.\n",
    "- Learning Rate: This controls the step-size taken during the optimization process; a smaller learning rate can help the model converge to a better solution.\n",
    "- Batch-Size: Refers to the number of examples processed within each training step. A larger batch size can speed up training, but results in a less accurate estimate of the gradient.\n",
    "- Sequence-Length: The length of the input sequence fed into an LSTM. A longer sequence can help the model capture long-term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cb49c6",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bafb9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_hyperparams = {\n",
    "    'input_size': 3,\n",
    "    'hidden_size': 128,\n",
    "    'num_hidden_layers': 6,\n",
    "    'batch_first': True,\n",
    "    'dropout': 0.5,\n",
    "    'bidirectional': False,\n",
    "    'output_size': 1,\n",
    "    'activation_function': nn.ReLU(),\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 0.1,\n",
    "    'momentum': 0.9,\n",
    "    'batch_size': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0dfd4aa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, rnn_hyperparams):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # Define the recurrent layers\n",
    "        self.rnn = nn.ModuleList()\n",
    "        self.rnn.append(nn.RNN(input_size=rnn_hyperparams['input_size'], \n",
    "                          hidden_size=rnn_hyperparams['hidden_size'],\n",
    "                          batch_first=rnn_hyperparams['batch_first'],\n",
    "                          dropout=rnn_hyperparams['dropout'],\n",
    "                          bidirectional=rnn_hyperparams['bidirectional']))\n",
    "        for i in range(1, rnn_hyperparams['num_hidden_layers']):\n",
    "            self.rnn.append(nn.RNN(input_size=rnn_hyperparams['hidden_size'], \n",
    "                          hidden_size=rnn_hyperparams['hidden_size'],\n",
    "                          batch_first=rnn_hyperparams['batch_first'],\n",
    "                          dropout=rnn_hyperparams['dropout'],\n",
    "                          bidirectional=rnn_hyperparams['bidirectional']))\n",
    "\n",
    "        # Define any additional layers, such as a fully connected output layer\n",
    "        self.fc = nn.Linear(rnn_hyperparams['hidden_size'], rnn_hyperparams['output_size'])\n",
    "        \n",
    "        # Saving some additional hyperparameters\n",
    "        self.activation_function = rnn_hyperparams['activation_function']\n",
    "        self.num_epochs = rnn_hyperparams['num_epochs']\n",
    "        self.learning_rate = rnn_hyperparams['learning_rate']\n",
    "        self.momentum = rnn_hyperparams['momentum']\n",
    "        self.batch_size = rnn_hyperparams['batch_size']\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass the input through the RNN\n",
    "        for rnn_layer in self.rnn:\n",
    "            x, hidden = rnn_layer(x)\n",
    "        \n",
    "        # Pass the final hidden state through a fully connected output layer\n",
    "        out = self.fc(hidden[-1])\n",
    "        \n",
    "        # Apply the activation function\n",
    "        out = self.activation_function(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12f77c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rgs88\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "RNN_Model_2 = RNN(rnn_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57bebb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): ModuleList(\n",
       "    (0): RNN(3, 128, batch_first=True, dropout=0.5)\n",
       "    (1): RNN(128, 128, batch_first=True, dropout=0.5)\n",
       "    (2): RNN(128, 128, batch_first=True, dropout=0.5)\n",
       "    (3): RNN(128, 128, batch_first=True, dropout=0.5)\n",
       "    (4): RNN(128, 128, batch_first=True, dropout=0.5)\n",
       "    (5): RNN(128, 128, batch_first=True, dropout=0.5)\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (activation_function): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN_Model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344beaf",
   "metadata": {},
   "source": [
    "Hyper-Parameter Tuning\n",
    "- Number of hidden layers\n",
    "- Learning Rate\n",
    "- Batch Size\n",
    "- Momentum\n",
    "- Activation Functions\n",
    "- Number of epochs\n",
    "- Momentum (idea behind this is to help smooth out variations in gradients which can occur during training, making it easier for the optimiser to find a good solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e81b1",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97891ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_hyperparams = {\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 500,\n",
    "    'learning_rate': 0.001,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 0.0001,\n",
    "    'activation_function' : nn.ReLU,\n",
    "    'num_hidden_layers': 4,\n",
    "    'dropout': 0.5,\n",
    "    'conv_layers': [\n",
    "        (32, 3, 1, 1),   # (out_channels, kernel_size, stride, padding)\n",
    "        (64, 3, 1, 1),\n",
    "        (128, 3, 1, 1)\n",
    "    ],\n",
    "    'fc_layers': [\n",
    "        (512, True),    # (out_features, use_relu)\n",
    "        (256, True),\n",
    "        (128, True),\n",
    "        (10, False)\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35cbf1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, cnn_hyperparams):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Extract hyperparameters\n",
    "        num_epochs = cnn_hyperparams['num_epochs']\n",
    "        batch_size = cnn_hyperparams['batch_size']\n",
    "        learning_rate = cnn_hyperparams['learning_rate']\n",
    "        momentum = cnn_hyperparams['momentum']\n",
    "        weight_decay = cnn_hyperparams['weight_decay']\n",
    "        dropout = cnn_hyperparams['dropout']\n",
    "        conv_layers = cnn_hyperparams['conv_layers']\n",
    "        fc_layers = cnn_hyperparams['fc_layers']\n",
    "        num_hidden_layers = cnn_hyperparams['num_hidden_layers']\n",
    "        activation_function = cnn_hyperparams['activation_function']\n",
    "\n",
    "        # Define convolutional layers\n",
    "        in_channels = 3\n",
    "        conv_modules = []\n",
    "        for out_channels, kernel_size, stride, padding in conv_layers:\n",
    "            conv_modules.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n",
    "            conv_modules.append(activation_function(inplace=True))\n",
    "            conv_modules.append(nn.BatchNorm2d(out_channels))\n",
    "            conv_modules.append(nn.Dropout2d(p=dropout))\n",
    "            in_channels = out_channels\n",
    "        self.conv = nn.Sequential(*conv_modules)\n",
    "\n",
    "        # Define fully connected layers\n",
    "        fc_modules = []\n",
    "        in_features = 128 * 32 * 32\n",
    "        for out_features, use_relu in fc_layers:\n",
    "            fc_modules.append(nn.Linear(in_features, out_features))\n",
    "            if use_relu:\n",
    "                fc_modules.append(activation_function(inplace=True))\n",
    "                fc_modules.append(nn.BatchNorm1d(out_features))\n",
    "                fc_modules.append(nn.Dropout(p=dropout))\n",
    "            in_features = out_features\n",
    "        self.fc = nn.Sequential(*fc_modules)\n",
    "\n",
    "        # Define output layer\n",
    "        self.output = nn.Linear(in_features, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f004ea",
   "metadata": {},
   "source": [
    "For CNNs, we can add the following layers:\n",
    "- Max-Pooling layer\n",
    "- CNN layer\n",
    "- ReLU layer\n",
    "- Fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1ae10",
   "metadata": {},
   "source": [
    "Hyper-Parameter Tuning for CNNs\n",
    "- Learning Rate\n",
    "- Batch Size\n",
    "- Number of epochs\n",
    "- Dropout rate\n",
    "- Number of filters (determines the complexity of the model - a high number of filters can capture more complex features)\n",
    "- Kernel Size (the size of the kernel put on the image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73489435",
   "metadata": {},
   "source": [
    "### Long-Short Term Memory Network (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c416dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_hyperparams = {\n",
    "    'input_size': 3,          # number of expected features in the input\n",
    "    'hidden_size': 256,         # number of features in the hidden state\n",
    "    'num_layers': 2,            # number of recurrent layers\n",
    "    'bidirectional': True,      # if True, becomes a bidirectional LSTM\n",
    "    'dropout': 0.2,             # dropout probability for the LSTM layers\n",
    "    'batch_first': True,        # if True, expects input tensors to have shape (batch_size, seq_length, input_size)\n",
    "    'batch_size': 64,           # mini-batch size for training\n",
    "    'seq_length': 20,           # length of input sequences\n",
    "    'learning_rate': 0.001,     # learning rate for the optimizer\n",
    "    'num_epochs': 10,           # number of epochs to train for\n",
    "    'num_classes': 10,          # number of classes in the output\n",
    "    'momentum': 0.9,\n",
    "    'activation_function': nn.ReLU,\n",
    "    'num_hidden_layers': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82191177",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, lstm_hyperparams):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = lstm_hyperparams['num_classes'] # number of classes\n",
    "        self.num_layers = lstm_hyperparams['num_layers'] # number of layers\n",
    "        self.input_size = lstm_hyperparams['input_size'] # input size\n",
    "        self.hidden_size = lstm_hyperparams['hidden_size'] # hidden state\n",
    "        self.seq_length = lstm_hyperparams['seq_length'] # sequence length\n",
    "        self.num_hidden_layers = lstm_hyperparams['num_hidden_layers'] # number of hidden layers\n",
    "        \n",
    "        # LSTM takes inputs, and outputs the hidden states with the dimensionality hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size = lstm_hyperparams['input_size'],\n",
    "                            hidden_size = lstm_hyperparams['hidden_size'],\n",
    "                            num_layers = lstm_hyperparams['num_layers'],\n",
    "                            bidirectional=lstm_hyperparams['bidirectional'],\n",
    "                            dropout = lstm_hyperparams['dropout'],\n",
    "                            batch_first = lstm_hyperparams['batch_first'])\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            self.hidden_layers.append(nn.Linear(lstm_hyperparams['hidden_size'], lstm_hyperparams['hidden_size']))\n",
    "        \n",
    "        self.fc = nn.Linear(lstm_hyperparams['hidden_size'], lstm_hyperparams['num_classes'])\n",
    "        \n",
    "        self.activation_function = lstm_hyperparams['activation_function']()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers * 2 if lstm_hyperparams['bidirectional'] else self.num_layers,\n",
    "                                   x.size(0),\n",
    "                                   self.hidden_size)) # hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers * 2 if lstm_hyperparams['bidirectional'] else self.num_layers,\n",
    "                                   x.size(0),\n",
    "                                   self.hidden_size)) # internal state\n",
    "        \n",
    "        # Propogate input through the LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) # LSTM with input, hidden, and internal state\n",
    "        \n",
    "        for i in range(self.num_hidden_layers):\n",
    "            output = self.hidden_layers[i](output)\n",
    "            output = self.activation_function(output)\n",
    "        \n",
    "        out = self.fc(output[:, -1, :])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566fc98a",
   "metadata": {},
   "source": [
    "The hyper-parameters we can modify for the LSTM are:\n",
    "- input size : the size of the input features for each step (this will be 4)\n",
    "- hidden size: the si\n",
    "\n",
    "input_size: The size of the input features for each time step. This is typically the number of features in your input data.\n",
    "\n",
    "hidden_size: The size of the hidden state and cell state for each LSTM unit. This controls the number of neurons in the LSTM layer.\n",
    "\n",
    "num_layers: The number of LSTM layers in the model. This determines the depth of the LSTM model.\n",
    "\n",
    "dropout: The dropout probability for the input and output of each LSTM layer. Dropout can help prevent overfitting.\n",
    "\n",
    "batch_first: If True, the input and output tensors are provided as (batch, seq_len, feature_dim). If False (default), the input and output tensors are provided as (seq_len, batch, feature_dim).\n",
    "\n",
    "bidirectional: If True, the LSTM layer is bidirectional. This means that there are two LSTM layers running in opposite directions, and the output at each time step is a concatenation of the outputs from both directions.\n",
    "\n",
    "bias: If False, the LSTM layers will not use bias weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb5f7b0",
   "metadata": {},
   "source": [
    "## Training the Models and Plotting accuracies and losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac85560",
   "metadata": {},
   "source": [
    "### Instantiating the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6bff3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating our first model as an object of the class\n",
    "RNN_Model_1 = RNN(rnn_hyperparams)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b17bc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating our second model as an object of the class\n",
    "CNN_Model_1 = CNN(cnn_hyperparams)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b73ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating our third model as an object of the class\n",
    "LSTM_Model_1 = LSTM(lstm_hyperparams)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6428055b",
   "metadata": {},
   "source": [
    "### Loss Criterion and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada1e406",
   "metadata": {},
   "source": [
    "Specifying the loss criterion and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e06a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the loss function and optimizer functions:\n",
    "#We will experiment with these three loss functions (each of these can be used for Regression Data)\n",
    "\n",
    "criterion1 = nn.MSELoss(reduction = 'mean')\n",
    "\n",
    "\n",
    "# criterion = nn.L1Loss() #same as MAE loss\n",
    "# criterion = nn.HuberLoss() #Combining MAE and MSE Losses\n",
    "\n",
    "#Negative Log-Likelihood loss function is only used for the softmax functions\n",
    "#Margin Ranking Loss Functions computers the criterion to predict relative distances between inputs (only used for Ranking)\n",
    "#Experimenting with three optimizers\n",
    "optimizer1 = torch.optim.SGD(RNN_Model_1.parameters(), lr=rnn_hyperparams['learning_rate'], momentum = rnn_hyperparams['momentum'])\n",
    "\n",
    "# optimizer = torch.optim.Adam(RNNmodel.paramaters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.RMSprop(RNNmodel.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7968c062",
   "metadata": {},
   "source": [
    "### Training Models and Evaluating Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea095ce",
   "metadata": {},
   "source": [
    "#### Using Skorch and Sci-Kit Learn with Pytorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "58d9be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skorch.callbacks import EarlyStopping, EpochScoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f36b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetRegressor(\n",
    "    RNN_Model_1,\n",
    "    criterion=nn.MSELoss,\n",
    "    optimizer=optim.SGD,\n",
    "    optimizer__momentum=rnn_hyperparams['momentum'],\n",
    "    lr=rnn_hyperparams['learning_rate'],\n",
    "    batch_size=1000,\n",
    "    max_epochs=rnn_hyperparams['num_epochs'],\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "564443b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rgs88\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1000, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\rgs88\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([809, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss       dur\n",
      "-------  ------------  ------------  --------\n",
      "      1        \u001b[36m0.0605\u001b[0m        \u001b[32m0.0601\u001b[0m  320.5140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rgs88\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([953, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.0598\u001b[0m        \u001b[32m0.0600\u001b[0m  309.8085\n",
      "      3        \u001b[36m0.0598\u001b[0m        \u001b[32m0.0600\u001b[0m  317.4145\n",
      "      4        \u001b[36m0.0598\u001b[0m        \u001b[32m0.0600\u001b[0m  368.1233\n",
      "      5        \u001b[36m0.0598\u001b[0m        \u001b[32m0.0600\u001b[0m  391.0365\n",
      "      6        \u001b[36m0.0598\u001b[0m        \u001b[32m0.0600\u001b[0m  393.8010\n",
      "      7        \u001b[36m0.0598\u001b[0m        \u001b[32m0.0600\u001b[0m  389.8979\n",
      "      8        \u001b[36m0.0598\u001b[0m        \u001b[32m0.0600\u001b[0m  370.3858\n",
      "      9        \u001b[36m0.0598\u001b[0m        \u001b[32m0.0600\u001b[0m  337.1983\n",
      "     10        \u001b[36m0.0598\u001b[0m        \u001b[32m0.0600\u001b[0m  313.2011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
       "  module_=RNN(\n",
       "    (rnn): ModuleList(\n",
       "      (0): RNN(3, 128, batch_first=True, dropout=0.5)\n",
       "      (1): RNN(128, 128, batch_first=True, dropout=0.5)\n",
       "      (2): RNN(128, 128, batch_first=True, dropout=0.5)\n",
       "      (3): RNN(128, 128, batch_first=True, dropout=0.5)\n",
       "      (4): RNN(128, 128, batch_first=True, dropout=0.5)\n",
       "      (5): RNN(128, 128, batch_first=True, dropout=0.5)\n",
       "    )\n",
       "    (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (activation_function): ReLU()\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X_train_t_1, y_train_t_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de35cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_t_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b774ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = net.predict(X_test_t_1)\n",
    "y_pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train_t_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce5c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean((y_pred_1 - y_test_1.to_numpy())**2)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57021d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd595223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test_1, y_pred_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238cd87a",
   "metadata": {},
   "source": [
    "Create grids and do gridsearch for all different hyperparameters for all different cnn Models\n",
    "Evaluate on different samples of data by company and by minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0bb9df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_hyperparameters = {\n",
    "    cnn_hyperparams['batch_size'] : [500, 1000, 2000],\n",
    "    cnn_hyperparams['momentum'] : [0.3, 1.0, 3.0],\n",
    "    cnn_hyperparams['num_epochs'] : [10, 25, 50],\n",
    "    cnn_hyperparams['activation_function'] : [nn.ReLU(), nn.LeakyReLU(), nn.ELU()],\n",
    "    cnn_hyperparams['learning_rate'] : [0.05, 0.2, 0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99970310",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_hyperparameters = {\n",
    "    rnn_hyperparams['num_epochs'] : [10, 25, 50],\n",
    "    rnn_hyperparams['learning_rate'] : [0.05, 0.1, 0.5],\n",
    "    rnn_hyperparams['momentum'] : [0.3, 1.0, 3.0],\n",
    "    rnn_hyperparams[\"batch_size\"] : [500, 1000, 2000],\n",
    "    rnn_hyperparams[\"activation_function\"] : [nn.ReLU, nn.LeakyReLU, nn.ELU()]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f8e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_hyperparameters = {\n",
    "    lstm_hyperparams[\"num_epochs\"] : [10,25,50],\n",
    "    lstm_hyperparams[\"learning_rate\"] : [0.05, 0.1, 0.5],\n",
    "    lstm_hyperparams[\"momentum\"] : [0.3, 1.0, 3.0],\n",
    "    lstm_hyperparams[\"batch_size\"] : [500, 1000, 2000],\n",
    "    lstm_hyperparams[\"activation_function\"] : [nn.ReLU, nn.LeakyReLU, nn.ELU()]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55264a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_gs = GridSearchCV(net, param_grid = cnn_hyperparameters, cv = 3, scoring = \"accuracy\", verbose = 1)\n",
    "rnn_gs = GridSearchCV(net, param_grid = rnn_hyperparameters, cv = 3, scoring = \"accuracy\", verbose = 1)\n",
    "lstm_gs = GridSearchCV(net, param_grid = lstm_hyperparameters, cv = 3, scoring = \"accuracy\", verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b166aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_gs.fit(X_val_t_1, y_val_t_1)\n",
    "rnn_gs.fit(X_val_t_1, y_val_t_1)\n",
    "lstm_gs.fit(X_val_t_1, y_val_t_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd06c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnn_gs.best_params)\n",
    "print(rnn_gs.best_params)\n",
    "print(lstm_gs.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score = net.score(X_test_t_1, y_test_t_1, scoring = 'r2')\n",
    "mae_score = net.score(X_test_t_1, y_test_t_1, scoring = 'neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 score:\", r2_score)\n",
    "print(\"MAE score:\", mae_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43836ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net.predict(X_test)\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Predicted vs. True Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb1aff",
   "metadata": {},
   "source": [
    "#### Pytorch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905de88",
   "metadata": {},
   "source": [
    "##### Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8111a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in train_loader1:\n",
    "    print(\"Inputs shape:\", inputs.shape)\n",
    "    print(\"Targets shape:\", targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae3e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn as nn\n",
    "\n",
    "import time\n",
    "import torch.nn as nn\n",
    "\n",
    "def train_model(criterion, optimizer, train_loader, test_loader, model):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(model.num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            print(inputs.size())\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Training Loss = {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_speed = end_time - start_time\n",
    "\n",
    "    # Test accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            total += targets.size(0)\n",
    "            loss = criterion(outputs, targets)\n",
    "            correct += loss.item()\n",
    "\n",
    "    test_loss = correct / total\n",
    "    print(f\"Test Loss = {test_loss:.4f}\")\n",
    "\n",
    "    print(f\"Training speed: {training_speed:.4f} seconds\")\n",
    "\n",
    "    return test_loss, training_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77fde96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss1, training_speed_1=train_model(criterion, optimizer, train_loader1, test_loader1, RNN_Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0cab7",
   "metadata": {},
   "source": [
    "##### Evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5495f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We then create a hyper-parameter grid and then run the model through the validation set to see which hyperparameters are the\n",
    "# best\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def validate_hyperparameters(val_loader):\n",
    "    \"\"\"\n",
    "    Validates hyperparameters using grid search on a validation data loader.\n",
    "    :param val_loader: PyTorch data loader for validation data\n",
    "    :return: dictionary of optimal hyperparameters\n",
    "    \"\"\"\n",
    "    # Define a grid of hyperparameters to search over\n",
    "    param_grid = {'learning_rate': [0.001, 0.01, 0.1],\n",
    "                  'num_layers': [1, 2, 3],\n",
    "                  'hidden_size': [32, 64, 128]}\n",
    "    \n",
    "    # Define a PyTorch model to train and validate\n",
    "    model = MyModel()\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize variables to store the optimal hyperparameters and accuracy\n",
    "    best_params = None\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # Iterate over the grid of hyperparameters\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        # Set the model hyperparameters\n",
    "        model.num_layers = params['num_layers']\n",
    "        model.hidden_size = params['hidden_size']\n",
    "        \n",
    "        # Set the optimizer hyperparameters\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "        \n",
    "        # Train the model on the training data for one epoch\n",
    "        num_epochs = 1\n",
    "        train_model(train_loader1, model, criterion, optimizer, num_epochs)\n",
    "        \n",
    "        # Evaluate the model on the validation data\n",
    "        acc = evaluate_model(val_loader, model)\n",
    "        \n",
    "        # If the accuracy is better than the current best, update the best hyperparameters and accuracy\n",
    "        if acc > best_acc:\n",
    "            best_params = params\n",
    "            best_acc = acc\n",
    "    \n",
    "    # Return the best hyperparameters as a dictionary\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bacfa93",
   "metadata": {},
   "source": [
    "##### Testing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cc229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "# We then update the hyperparameters before performing a final evaluation of the model.\n",
    "def evaluate_model(data_loader, model):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a PyTorch model on a validation data loader.\n",
    "    :param data_loader: PyTorch data loader for validation data\n",
    "    :param model: PyTorch model to evaluate\n",
    "    :return: a dictionary containing the accuracy, F1 score, and recall of the model's predictions\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize variables to keep track of the total number of correct predictions and the total number of examples\n",
    "    num_correct = 0\n",
    "    num_examples = 0\n",
    "    \n",
    "    # Initialize variables to keep track of true positives, false positives, and false negatives\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    # Iterate over the data loader\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            # Move the inputs and labels to the device (e.g. GPU) being used\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Compute the outputs of the model\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute the predicted classes as the indices of the maximum output values\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Update the total number of correct predictions and examples\n",
    "            num_correct += (predicted == labels).sum().item()\n",
    "            num_examples += labels.size(0)\n",
    "            \n",
    "            # Update the true positives, false positives, and false negatives\n",
    "            true_positives += ((predicted == 1) & (labels == 1)).sum().item()\n",
    "            false_positives += ((predicted == 1) & (labels == 0)).sum().item()\n",
    "            false_negatives += ((predicted == 0) & (labels == 1)).sum().item()\n",
    "    \n",
    "    # Compute the accuracy as the ratio of correct predictions to total examples\n",
    "    accuracy = num_correct / num_examples\n",
    "    \n",
    "    # Compute the F1 score\n",
    "    f1 = f1_score(labels.cpu().numpy(), predicted.cpu().numpy())\n",
    "    \n",
    "    # Compute the recall\n",
    "    recall = recall_score(labels.cpu().numpy(), predicted.cpu().numpy())\n",
    "    \n",
    "    # Return the accuracy, F1 score, and recall in a dictionary\n",
    "    return {'accuracy': accuracy, 'f1': f1, 'recall': recall}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8326b3e3",
   "metadata": {},
   "source": [
    "## Results/ Resulting Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7a52b4",
   "metadata": {},
   "source": [
    "We then plot the f1 score, accuracy, precision, training speed, test loss on for all different types of models. This can then be used to determine which model is the best.\n",
    "One line graph/ bar chart for accuracy, precision, training speed, f1 loss and test loss on all the models.\n",
    "Finally, we do further investigation to see which model is better on which time range, with the dataset above, and which is better for each company; if there are any differences. So we run these tests again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a996c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_accuracies = \n",
    "RNN_accuracies = \n",
    "LSTM_accuracies ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(history):\n",
    "    \"\"\" Plot the history of accuracies\"\"\"\n",
    "    accuracies = [x['val_acc'] for x in history]\n",
    "    plt.plot(accuracies, '-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb1b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b839151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    \"\"\" Plot the losses in each epoch\"\"\"\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8772dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22964f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
