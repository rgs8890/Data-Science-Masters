{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92cac8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ccd380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ICICI_min= pd.read_csv(r\"C:\\Users\\rgs88\\OneDrive\\Documents\\Semester 2 Data Science Masters\\Neural Computing\\NN Final Project\\ICICI_min_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb26181",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ICICI_min = data_ICICI_min.drop(columns = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0771b52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.209937</td>\n",
       "      <td>0.210460</td>\n",
       "      <td>0.207598</td>\n",
       "      <td>0.206675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.206229</td>\n",
       "      <td>0.206339</td>\n",
       "      <td>0.206649</td>\n",
       "      <td>0.206091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.205996</td>\n",
       "      <td>0.206158</td>\n",
       "      <td>0.206714</td>\n",
       "      <td>0.206377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.206579</td>\n",
       "      <td>0.206339</td>\n",
       "      <td>0.206766</td>\n",
       "      <td>0.205909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.205931</td>\n",
       "      <td>0.205691</td>\n",
       "      <td>0.205713</td>\n",
       "      <td>0.204845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671085</th>\n",
       "      <td>0.990210</td>\n",
       "      <td>0.989504</td>\n",
       "      <td>0.992850</td>\n",
       "      <td>0.990722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671086</th>\n",
       "      <td>0.990146</td>\n",
       "      <td>0.991059</td>\n",
       "      <td>0.993305</td>\n",
       "      <td>0.992020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671087</th>\n",
       "      <td>0.991313</td>\n",
       "      <td>0.991512</td>\n",
       "      <td>0.994280</td>\n",
       "      <td>0.992279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671088</th>\n",
       "      <td>0.992220</td>\n",
       "      <td>0.992938</td>\n",
       "      <td>0.995450</td>\n",
       "      <td>0.994290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671089</th>\n",
       "      <td>0.993971</td>\n",
       "      <td>0.995788</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>0.997210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>671090 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            open      high       low     close\n",
       "0       0.209937  0.210460  0.207598  0.206675\n",
       "1       0.206229  0.206339  0.206649  0.206091\n",
       "2       0.205996  0.206158  0.206714  0.206377\n",
       "3       0.206579  0.206339  0.206766  0.205909\n",
       "4       0.205931  0.205691  0.205713  0.204845\n",
       "...          ...       ...       ...       ...\n",
       "671085  0.990210  0.989504  0.992850  0.990722\n",
       "671086  0.990146  0.991059  0.993305  0.992020\n",
       "671087  0.991313  0.991512  0.994280  0.992279\n",
       "671088  0.992220  0.992938  0.995450  0.994290\n",
       "671089  0.993971  0.995788  0.996750  0.997210\n",
       "\n",
       "[671090 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ICICI_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2bdf162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=[\"close\"]), data[\"close\"], test_size=test_ratio, random_state=random_state)\n",
    "\n",
    "    # Split the remaining data into train and validation sets\n",
    "    remaining_ratio = 1 - test_ratio\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_ratio/remaining_ratio, random_state=random_state)\n",
    "\n",
    "    # Convert the output to pandas DataFrame\n",
    "    X_train = pd.DataFrame(X_train, columns=data.drop(columns=[\"close\"]).columns)\n",
    "    y_train = pd.DataFrame(y_train, columns=[\"close\"])\n",
    "    X_val = pd.DataFrame(X_val, columns=data.drop(columns=[\"close\"]).columns)\n",
    "    y_val = pd.DataFrame(y_val, columns=[\"close\"])\n",
    "    X_test = pd.DataFrame(X_test, columns=data.drop(columns=[\"close\"]).columns)\n",
    "    y_test = pd.DataFrame(y_test, columns=[\"close\"])\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17efb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, y_train_1, X_val_1, y_val_1, X_test_1, y_test_1 = split_data(ICICI_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f754ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def convert_to_tensors(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    X_train_np = X_train.values\n",
    "    X_val_np = X_val.values\n",
    "    X_test_np = X_test.values\n",
    "    y_train_np = y_train.values\n",
    "    y_val_np = y_val.values\n",
    "    y_test_np = y_test.values\n",
    "\n",
    "    X_train_t = torch.from_numpy(X_train_np).type(torch.Tensor)\n",
    "    X_val_t = torch.from_numpy(X_val_np).type(torch.Tensor)\n",
    "    X_test_t = torch.from_numpy(X_test_np).type(torch.Tensor)\n",
    "    y_train_t = torch.from_numpy(y_train_np).type(torch.Tensor)\n",
    "    y_val_t = torch.from_numpy(y_val_np).type(torch.Tensor)\n",
    "    y_test_t = torch.from_numpy(y_test_np).type(torch.Tensor)\n",
    "\n",
    "    return X_train_t, y_train_t, X_val_t, y_val_t, X_test_t, y_test_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b1d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t_1, y_train_t_1, X_val_t_1, y_val_t_1, X_test_t_1, y_test_t_1 = convert_to_tensors(X_train_1, y_train_1, X_val_1, y_val_1, X_test_1, y_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d8c3d0",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46f766b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m rnn_hyperparams \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_hidden_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m6\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_first\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbidirectional\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation_function\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mReLU,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3.0\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2000\u001b[39m\n\u001b[0;32m     14\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "rnn_hyperparams = {\n",
    "    'input_size': 3,\n",
    "    'hidden_size': 128,\n",
    "    'num_hidden_layers': 6,\n",
    "    'batch_first': True,\n",
    "    'dropout': 0.5,\n",
    "    'bidirectional': False,\n",
    "    'output_size': 1,\n",
    "    'activation_function': nn.ReLU,\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 0.5,\n",
    "    'momentum': 3.0,\n",
    "    'batch_size': 2000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, rnn_hyperparams):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # Define the recurrent layers\n",
    "        self.rnn = nn.ModuleList()\n",
    "        self.rnn.append(nn.RNN(input_size=rnn_hyperparams['input_size'], \n",
    "                          hidden_size=rnn_hyperparams['hidden_size'],\n",
    "                          batch_first=rnn_hyperparams['batch_first'],\n",
    "                          dropout=rnn_hyperparams['dropout'],\n",
    "                          bidirectional=rnn_hyperparams['bidirectional']))\n",
    "        for i in range(1, rnn_hyperparams['num_hidden_layers']):\n",
    "            self.rnn.append(nn.RNN(input_size=rnn_hyperparams['hidden_size'], \n",
    "                          hidden_size=rnn_hyperparams['hidden_size'],\n",
    "                          batch_first=rnn_hyperparams['batch_first'],\n",
    "                          dropout=rnn_hyperparams['dropout'],\n",
    "                          bidirectional=rnn_hyperparams['bidirectional']))\n",
    "\n",
    "        # Define any additional layers, such as a fully connected output layer\n",
    "        self.fc = nn.Linear(rnn_hyperparams['hidden_size'], rnn_hyperparams['output_size'])\n",
    "        \n",
    "        # Saving some additional hyperparameters\n",
    "        self.activation_function = rnn_hyperparams['activation_function']\n",
    "        self.num_epochs = rnn_hyperparams['num_epochs']\n",
    "        self.learning_rate = rnn_hyperparams['learning_rate']\n",
    "        self.momentum = rnn_hyperparams['momentum']\n",
    "        self.batch_size = rnn_hyperparams['batch_size']\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass the input through the RNN\n",
    "        for rnn_layer in self.rnn:\n",
    "            x, hidden = rnn_layer(x)\n",
    "        \n",
    "        # Pass the final hidden state through a fully connected output layer\n",
    "        out = self.fc(hidden[-1])\n",
    "        \n",
    "        # Apply the activation function\n",
    "        out = self.activation_function(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9cfd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_hyperparams = {\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 2000,\n",
    "    'learning_rate': 0.5,\n",
    "    'momentum': 1.0,\n",
    "    'weight_decay': 0.0001,\n",
    "    'activation_function' : nn.ELU,\n",
    "    'num_hidden_layers': 4,\n",
    "    'dropout': 0.5,\n",
    "    'conv_layers': [\n",
    "        (32, 3, 1, 1),   # (out_channels, kernel_size, stride, padding)\n",
    "        (64, 3, 1, 1),\n",
    "        (128, 3, 1, 1)\n",
    "    ],\n",
    "    'fc_layers': [\n",
    "        (512, True),    # (out_features, use_relu)\n",
    "        (256, True),\n",
    "        (128, True),\n",
    "        (10, False)\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad12dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, cnn_hyperparams):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Extract hyperparameters\n",
    "        num_epochs = cnn_hyperparams['num_epochs']\n",
    "        batch_size = cnn_hyperparams['batch_size']\n",
    "        learning_rate = cnn_hyperparams['learning_rate']\n",
    "        momentum = cnn_hyperparams['momentum']\n",
    "        weight_decay = cnn_hyperparams['weight_decay']\n",
    "        dropout = cnn_hyperparams['dropout']\n",
    "        conv_layers = cnn_hyperparams['conv_layers']\n",
    "        fc_layers = cnn_hyperparams['fc_layers']\n",
    "        num_hidden_layers = cnn_hyperparams['num_hidden_layers']\n",
    "        activation_function = cnn_hyperparams['activation_function']\n",
    "\n",
    "        # Define convolutional layers\n",
    "        in_channels = 3\n",
    "        conv_modules = []\n",
    "        for out_channels, kernel_size, stride, padding in conv_layers:\n",
    "            conv_modules.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n",
    "            conv_modules.append(activation_function(inplace=True))\n",
    "            conv_modules.append(nn.BatchNorm2d(out_channels))\n",
    "            conv_modules.append(nn.Dropout2d(p=dropout))\n",
    "            in_channels = out_channels\n",
    "        self.conv = nn.Sequential(*conv_modules)\n",
    "\n",
    "        # Define fully connected layers\n",
    "        fc_modules = []\n",
    "        in_features = 128 * 32 * 32\n",
    "        for out_features, use_relu in fc_layers:\n",
    "            fc_modules.append(nn.Linear(in_features, out_features))\n",
    "            if use_relu:\n",
    "                fc_modules.append(activation_function(inplace=True))\n",
    "                fc_modules.append(nn.BatchNorm1d(out_features))\n",
    "                fc_modules.append(nn.Dropout(p=dropout))\n",
    "            in_features = out_features\n",
    "        self.fc = nn.Sequential(*fc_modules)\n",
    "\n",
    "        # Define output layer\n",
    "        self.output = nn.Linear(in_features, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_hyperparams = {\n",
    "    'input_size': 3,          # number of expected features in the input\n",
    "    'hidden_size': 256,         # number of features in the hidden state\n",
    "    'num_layers': 2,            # number of recurrent layers\n",
    "    'bidirectional': True,      # if True, becomes a bidirectional LSTM\n",
    "    'dropout': 0.5,             # dropout probability for the LSTM layers\n",
    "    'batch_first': True,        # if True, expects input tensors to have shape (batch_size, seq_length, input_size)\n",
    "    'batch_size': 2000,           # mini-batch size for training\n",
    "    'seq_length': 20,           # length of input sequences\n",
    "    'learning_rate': 0.001,     # learning rate for the optimizer\n",
    "    'num_epochs': 10,           # number of epochs to train for\n",
    "    'num_classes': 10,          # number of classes in the output\n",
    "    'momentum': 0.9,\n",
    "    'activation_function': nn.ReLU,\n",
    "    'num_hidden_layers': [15,30]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeea156",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, lstm_hyperparams):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = lstm_hyperparams['num_classes'] # number of classes\n",
    "        self.num_layers = lstm_hyperparams['num_layers'] # number of layers\n",
    "        self.input_size = lstm_hyperparams['input_size'] # input size\n",
    "        self.hidden_size = lstm_hyperparams['hidden_size'] # hidden state\n",
    "        self.seq_length = lstm_hyperparams['seq_length'] # sequence length\n",
    "        self.num_hidden_layers = lstm_hyperparams['num_hidden_layers'] # number of hidden layers\n",
    "        \n",
    "        # LSTM takes inputs, and outputs the hidden states with the dimensionality hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size = lstm_hyperparams['input_size'],\n",
    "                            hidden_size = lstm_hyperparams['hidden_size'],\n",
    "                            num_layers = lstm_hyperparams['num_layers'],\n",
    "                            bidirectional=lstm_hyperparams['bidirectional'],\n",
    "                            dropout = lstm_hyperparams['dropout'],\n",
    "                            batch_first = lstm_hyperparams['batch_first'])\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            self.hidden_layers.append(nn.Linear(lstm_hyperparams['hidden_size'], lstm_hyperparams['hidden_size']))\n",
    "        \n",
    "        self.fc = nn.Linear(lstm_hyperparams['hidden_size'], lstm_hyperparams['num_classes'])\n",
    "        \n",
    "        self.activation_function = lstm_hyperparams['activation_function']()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers * 2 if lstm_hyperparams['bidirectional'] else self.num_layers,\n",
    "                                   x.size(0),\n",
    "                                   self.hidden_size)) # hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers * 2 if lstm_hyperparams['bidirectional'] else self.num_layers,\n",
    "                                   x.size(0),\n",
    "                                   self.hidden_size)) # internal state\n",
    "        \n",
    "        # Propogate input through the LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) # LSTM with input, hidden, and internal state\n",
    "        \n",
    "        for i in range(self.num_hidden_layers):\n",
    "            output = self.hidden_layers[i](output)\n",
    "            output = self.activation_function(output)\n",
    "        \n",
    "        out = self.fc(output[:, -1, :])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738cd111",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating our first model as an object of the class\n",
    "RNN_Model_Final = RNN(rnn_hyperparams)\n",
    "#\n",
    "#Instantiating our second model as an object of the class\n",
    "CNN_Model_Final = CNN(cnn_hyperparams)\n",
    "#\n",
    "#Instantiating our third model as an object of the class\n",
    "LSTM_Model_FInal = LSTM(lstm_hyperparams)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d3b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion1 = nn.MSELoss(reduction = 'mean')\n",
    "optimizer1 = torch.optim.SGD(RNN_Model_1.parameters(), lr=rnn_hyperparams['learning_rate'], momentum = rnn_hyperparams['momentum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f22a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skorch.callbacks import EarlyStopping, EpochScoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0c2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetRegressor(\n",
    "    RNN_Model_1,\n",
    "    criterion=nn.MSELoss,\n",
    "    optimizer=optim.SGD,\n",
    "    optimizer__momentum=rnn_hyperparams['momentum'],\n",
    "    lr=rnn_hyperparams['learning_rate'],\n",
    "    batch_size=1000,\n",
    "    max_epochs=rnn_hyperparams['num_epochs'],\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf81ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(X_train_t_1, y_train_t_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean((y_pred_1 - y_test_1.to_numpy())**2)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
